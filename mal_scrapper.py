#!/usr.bin/env python3
##
# Dakara Project
#

import bs4 as bs
import json
import html
import re
import requests

JSON_DEFAULT_PATH = './mal_scrapper.json'


class File():
    def __init__(self, filename, mode):
        self.filename = filename
        self.mode = mode

    def __enter__(self):
        self.file = open(self.filename, self.mode)
        return self.file

    def __exit__(self, *args):
        self.file.close()


def get_alternative_titles(tags, force_scrap=False):
    """
    Find and save the alternative titles of an anime as found on MyAnimeList.
    The scrapped data will be cached to reduce later parsing times.
    tags is a dictionnary generated by anime_parse.parse_file_name
    """
    with File(json_file_path, 'r') as json_file:
        saved_data = json.load(json_file)

    name = tags['title_work']
    if tags['subtitle_work'] != '':
        name = name + ' ~ ' + tags['subtitle_work']

    # If we don't find the anime, let's scrap MAL
    scrapped = False
    if name not in saved_data or force_scrap:  # We check if we don't already have the anime scrapped
        scrap_anime(name, saved_data)
        scrapped = True

    # If we find the anime, let's get the alternative titles
    if name in saved_data:
        anime = saved_data[name]

        if 'alternative_titles' in anime.keys():  # Index in list, we use it
            return anime['alternative_titles']

        if not scrapped:  # Index not in list but no scrapping done... Local data may not be updated so we do it now
            return get_alternative_titles(tags, True)

    return []


def get_artists(tags, force_scrap=False):
    """
    Find and save the artists of an anime theme by scrapping MyAnimeList.
    The scrapped data will be cached to reduce later parsing times.
    tags is a dictionnary generated by anime_parse.parse_file_name (will be edited with the found artists)
    """

    # We don't care about IS
    if tags['link_type'] not in ['OP', 'ED']:
        return []

    # json_file_path = os.path.dirname(os.path.realpath(__file__)) + '/mal_scrapper.json'

    # We first load the local data
    with File(json_file_path, 'r') as json_file:
        saved_data = json.load(json_file)

    name = tags['title_work']
    if tags['subtitle_work'] != '':
        name = name + ' ~ ' + tags['subtitle_work']

    # If we don't find the anime, let's scrap MAL
    scrapped = False
    if name not in saved_data or force_scrap:  # We check if we don't already have the anime scrapped
        scrap_anime(name, saved_data)
        scrapped = True

    # If we find the anime, let's get the artists
    if name in saved_data:
        anime = saved_data[name]
        index = str(tags['link_nb'])
        if index == '0': # If the index either 0, it will still be stored as 1
            index = '1'

        if index in anime[tags['link_type']]:  # Index in list, we use it
            return anime[tags['link_type']][index]

        if not scrapped:  # Index not in list but no scrapping done... Local data may not be updated so we do it now
            return get_artists(tags, True)

        # Index not in list despite scrapping... Can't do anything else here
        print('\nWARNING MAL_Scrapper: ' + name + ' ' + tags['link_type'] + str(tags['link_nb']) + ' not found')

    return []


def scrap_anime(title, data, query, json_file_path=JSON_DEFAULT_PATH, subtitle=""):
    """
    This function does the actual scrapping and saves it in data.
    """

    # First steap: finding the page of the anime, using the search engine
    response = requests.get('https://myanimelist.net/anime.php?q="' + query + '"')
    if not response.ok:
        print('\nWARNING MAL_Scrapper: can\'t connect to MyAnimeList')
        return

    found = {}

    kept_results_max = 10  # Max number of results kept

    parsed = bs.BeautifulSoup(response.text, features='html.parser')
    animes_list = parsed.find('div', {'class': 'js-block-list'}).table.findAll('tr')

    anime_found = False
    for i in range(1, kept_results_max + 1):  # We won't keep more than kept_results_max entries
        anime = html.unescape(animes_list[i].find('strong').string)
        link = animes_list[i].find('a', {'class': 'hoverinfo_trigger'})['href']
        found[anime] = link  # We store the result and continue

        if anime == query: # If we find a perfect match, that's great
            anime_found = True
            break

        anime_match = re.sub('[?:]', '', anime)
        anime_match = re.sub(' - ', ' ~ ', anime_match)
        anime_match = re.sub('//', ' ', anime_match)
        anime_match = re.sub('/', ' ', anime_match)

        if anime_match == query:  # If we find a perfect match, that's great!
            anime_found = True
            break

    if not anime_found:  # If we didn't get a perfect match, we will ask the user
        animes_found = list(found)
        print()
        for i in range(len(animes_found)):
            print(str(i) + ": " + animes_found[i])
        match_index = int(input(query + " : which is the right one? "))
        if (match_index >= kept_results_max):  # No match, no need to continue
            return

        anime = animes_found[match_index]

    # Second step: getting the page of the found anime
    r = requests.get(found[anime])
    parsed = bs.BeautifulSoup(r.text, features='html.parser')

    # Third step: extracting the data we want (artists, alternative titles, ...)
    op = extract_artists(parsed.find('div', {'class': 'opnening'}))
    ed = extract_artists(parsed.find('div', {'class': 'ending'}))
    alternative_titles = extract_alternative_titles(parsed.findAll('div', {'class': 'spaceit_pad'}))

    # Fourth step: saving the data locally
    name = title
    if subtitle:
        name = title + " ~ " + subtitle

    data[name] = {}
    data[name]['OP'] = op
    data[name]['ED'] = ed
    data[name]['alternative_titles'] = alternative_titles
    data[name]['mal_title'] = anime
    data[name]['link'] = found[anime]

    with File(json_file_path, 'w') as json_file:
        json.dump(data, json_file, indent=2, sort_keys=True)


def extract_artists(tag):
    """
    When scrapping, extracts the artists from the div that contains them.
    """
    artists = {}
    for entry in tag.findAll('span', {'class': 'theme-song'}):  # For each theme
        temp_artists = []
        number = re.sub('^#0*([0-9]+).*$', r'\1', entry.string)  # This gets the id of the theme
        if number == entry.string:
            number = str(len(artists) + 1)
        found = re.sub('^.*[\"\)]:? *by (.*)$', r'\1', html.unescape(entry.string))  # This gets the artists (by finding 'by')
        if found == html.unescape(entry.string):
            found = re.sub('^.*[\"\)]:? *composed by (.*)$', r'\1', html.unescape(entry.string))
            if found == html.unescape(entry.string):
                found = re.sub('^.*[\"\)]:? *- (.*)$', r'\1', html.unescape(entry.string))
                if found == html.unescape(entry.string):
                    found = ''
        found = re.sub(' \( *[eE]p[^\(]*\)\.?$', '', found)  # This removes the information about the episodes (by finding 'ep')
        found = re.sub(' \(TV.*\)$', '', found)  # This removes parenthesis begining by TV
        found = re.sub(' \(BD.*\)$', '', found)  # This removes parenthesis begining by BD
        found = re.sub(' \(DVD.*\)$', '', found)  # This removes parenthesis begining by DVD
        found = found.encode("ascii", errors="ignore").decode()  # This removes the non ASCII characters
        found = re.sub(' \( *\)', '', found)  # This removes the parenthesis with spaces only (after non ASCII deletion)
        for artist in found.split(', '):  # Here we split if there are multiple artists
            artist = re.sub('^.* \((.*)\)', r'\1', artist)  # For the cases where the real artist is in parenthesis
            artist = re.sub('^CV: ', '', artist)  # For the cases where the artist begins by CV
            artist = re.sub('^Seiyuu: ', '', artist)  # For the cases where the artist begins by Seiyuu
            temp_artists.append(artist)
        artists[number] = temp_artists

    return artists


def extract_alternative_titles(tags):
    """
    When scrapping, extracts the alternative titles from the div that contains them.
    """
    alternative_titles = []
    for tag in tags:
        if tag.find('span', {'class': 'dark_text'}):
            found = html.unescape(re.sub('^[^:]*: ', '', tag.text))
            found = re.sub('\n.*$', '', found)
            alternative_titles += found.split(', ')

    return alternative_titles
